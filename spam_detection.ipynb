{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d98a4e-9418-4397-8b7a-8088621543bc",
   "metadata": {},
   "source": [
    "# Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd6799-47e0-46c6-b8e3-64a0cb60587c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Spamming is one of the simplest attacks in email communication. Users often receive annoying spam messages and malicious phishing emails by subscribing to various websites, products, services, directories, newsletters, and other types of electronic communication. In some cases, spam emails are generated by viruses or trojan horses sent en masse.\n",
    "\n",
    "There are many solutions for spam filtering, such as blacklisting and whitelisting techniques, decision tree-based approaches, email address-based approaches, and machine learning-based methods. Most of them rely mainly on the analysis of the email content's text. As a result, there is an increasing demand for effective anti-spam filters that automatically identify and remove spam messages or warn users about potential spam messages. However, spammers always explore the gaps in existing spam filtering techniques and introduce new designs to spread spam widely, e.g., the tokenization attack sometimes misleads spam filters by adding extra spaces. Therefore, the content of emails needs to be structured. Moreover, despite having the highest accuracy in detecting spam using machine learning, false positives (FP) are a problem due to one-time email threat detection. To address the issues of false positives and changes in various attack designs, keywords and other unwanted information are removed from the text before further analysis. After preprocessing, these texts go through numerous feature extraction methods, such as word2vec, word n-gram, character n-gram, and combinations of n-grams with variable lengths. Various machine learning techniques, such as support vector machine (SVM), decision tree (DT), logistic regression (LR), and multinomial naive Bayes (MNB), are used to classify emails.\n",
    "\n",
    "In this task, we will focus only on the naive Bayes classifier method presented in the lecture along with the [multinomial version](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550496e-1cac-4071-935a-6f92eec8afd7",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers (NB)\n",
    "\n",
    "In our experiments, after preprocessing, each message is ultimately represented as a vector $\\mathbf{x}=(x_1, \\ldots , x_m)$, where $x_1, \\ldots , x_m$ are the attribute values $X_1, \\ldots , X_m$, and each attribute provides information about a specific token of the message. In the simplest case, all attributes are boolean values: $X_i = 1$ if the message contains the given token; otherwise, $X_i = 0$. Alternatively, their values can be token frequencies (TF), showing how many times the corresponding token appears in the message. Attributes with TF values carry more information than boolean attributes.\n",
    "\n",
    "According to Bayes' theorem, the probability that a message with vector $\\mathbf{x} = (x_1, \\ldots, x_m)$ belongs to category $c$ is:\n",
    "\n",
    "$$\n",
    "p(c | \\mathbf{x}) = \\frac{p(c) \\cdot p(\\mathbf{x} | c)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Since the denominator does not depend on the category, the NB classifier classifies each message to the category that maximizes $p(c) \\cdot p(\\mathbf{x} | c)$. In the case of spam filtering, this means classifying a message as spam when:\n",
    "\n",
    "$$\n",
    "\\frac{p(c_s) \\cdot p(\\mathbf{x} | c_s)}{p(c_s) \\cdot p(\\mathbf{x} | c_s) + p(c_h) \\cdot p(\\mathbf{x} | c_h)} > T\n",
    "$$\n",
    "\n",
    "where $T = 0.5$, and $c_h$ and $c_s$ denote the categories ham and spam. By changing $T$, one can choose to have more true negatives (correctly classified ham messages) at the cost of fewer true positives (correctly classified spam messages), or vice versa. The prior probabilities $p(c)$ are usually estimated by dividing the number of training messages in category $c$ by the total number of training messages. The probabilities $p(\\mathbf{x} | c)$ are estimated differently in each version of NB - see the lecture.\n",
    "\n",
    "## Multinomial Naive Bayes Classifier (MNB)\n",
    "\n",
    "The [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) naive Bayes classifier with TF attributes treats each message $d$ as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) containing each token $t_i$ as many times as it appears in $d$. Therefore, $d$ can be represented as $\\mathbf{x} = (x_1, ..., x_m)$, where each $x_i$ is now the number of occurrences of $t_i$ in $d$. Additionally, each message $d$ from category $c$ is viewed as a result of independently choosing $|d|$ tokens from $F=\\{t_1,\\ldots,t_m\\}$ with replacement, with probability $p(t_i | c)$ for each $t_i$. Then $p(\\mathbf{x} | c)$ is the multinomial distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid c) = p(|d|) \\cdot |d|! \\cdot \\prod_{i=1}^{d} \\frac{p(t_i \\mid c)^{x_i}}{x_i !}\n",
    "$$\n",
    "\n",
    "where we assume that $|d|$ does not depend on the category $c$. This is an additional simplifying assumption that is more debatable in spam filtering. For example, the probability of receiving a very long spam message seems smaller than the probability of receiving an equally long ham message. The criterion for classifying a message as spam becomes:\n",
    "\n",
    "$$\n",
    "\\frac{p(c_s) \\cdot \\prod_{i=1}^{m} p(t_i \\mid c_s)^{x_i}}{p(c_s)\\cdot\\prod_{i=1}^{m} p(t_i \\mid c_s)^{x_i} + p(c_h)\\cdot\\prod_{i=1}^{m} p(t_i \\mid c_h)^{x_i}}  > T\n",
    "$$\n",
    "\n",
    "where each $p(t_i | c)$ is estimated as:\n",
    "\n",
    "$$\n",
    "p(t \\mid c) = \\frac{\\alpha + N_{t,c}}{\\alpha \\cdot m + N_c}\n",
    "$$\n",
    "\n",
    "where $N_{t,c}$ is the number of occurrences of token $t$ in the training messages of category $c$, while $N_c = \\sum_{i=1}^{m} N_{t_i,c}$ is the total number of messages in the training set of category $c$. In practice, a parameter $\\alpha$ is added, which represents smoothing and solves the problem of zero probability, see [http://www.paulgraham.com/spam.html](http://www.paulgraham.com/spam.html) (e.g., $\\alpha=1$).\n",
    "\n",
    "### Example Multinomial Data\n",
    "\n",
    "Thus, each message $d$ consists of different tokens $t_i$, and each of these $t_i$ belongs to the dictionary $\\mathcal{V}$. If $\\mathcal{V}$ contains, for example, $8$ tokens, $t_1,t_2,...,t_8$, and the message is: $t_1 t_2 t_2 t_6 t_3 t_2 t_8$, the representation of this message will be as follows:\n",
    "\n",
    "| |$t_1$|$t_2$|$t_3$|$t_4$|$t_5$|$t_6$|$t_7$|$t_8$|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|$\\mathbf{x}$| 1|3 |1 | 0| 0|1 | 0|1 |\n",
    "\n",
    "After adding a few other random messages, the dataset looks like this:\n",
    "\n",
    "|$t_1$|$t_2$|$t_3$|$t_4$|$t_5$|$t_6$|$t_7$|$t_8$|$c$|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| 1|3 |1 | 0| 0|1 | 0|1 | spam|\n",
    "| 1|0 |0 | 0| 1|1 | 1|3 | ham|\n",
    "| 0|0 |0 | 0| 0|2 | 1|2 | spam|\n",
    "\n",
    "Assuming the classes ($1$-spam, $0$-ham), we have $c = [1,0,1]$. Now, comparing with the equation above:\n",
    "\n",
    "- $N_{t_i,c}$ is the number of occurrences of feature $t_i$ in each unique class $c$. For example, for $c=1$, $N_{t_1,c}=1, N_{t_6,c}=3$\n",
    "- $N_c$ is the total number of occurrences of all features in each unique class $c$. For example, for $c=1$, $N_c=12$\n",
    "- $m=8$ is the total number of features\n",
    "- $\\alpha=1$ is known as the smoothing parameter. It is needed to address the zero probability problem (see [http://www.paulgraham.com/spam.html](http://www.paulgraham.com/spam.html))\n",
    "\n",
    "## Floating Point Underflow\n",
    "\n",
    "To avoid the problem of floating point underflow, multiplying a set of small probabilities, i.e., simply the product becomes too small to represent and is replaced by 0. Instead of calculating\n",
    "\n",
    "$$\n",
    "P(c) \\prod_{i=1}^m P(t_i | c)\n",
    "$$\n",
    "\n",
    "which may cause underflow, consider calculating the logarithm of this expression,\n",
    "\n",
    "$$\n",
    "\\log\\left(P(c) \\prod_{i=1}^m P(t_i | c)\\right)\n",
    "$$\n",
    "\n",
    "which equivalently can be written as\n",
    "\n",
    "$$\n",
    "\\log(P(c))+ \\sum_{i=1}^m \\log(P(t_i | c))\n",
    "$$\n",
    "\n",
    "Then notice that if\n",
    "\n",
    "$$\n",
    "\\log(P(c_s))+ \\sum_{i=1}^m \\log(P(t_i | c_s)) > \\log(P(c_h))+ \\sum_{i=1}^m \\log(P(t_i | c_h))\n",
    "$$\n",
    "\n",
    "then, since $\\log(x) > \\log(y)$ if and only if $x > y$, it follows that\n",
    "\n",
    "$$\n",
    "P(c_s) \\prod_{i=1}^m P(t_i | c_s) > P(c_h) \\prod_{i=1}^m P(t_i | c_h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff1e5a-8599-412a-9494-45699a65a2fb",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "### Classifier Based on the NB Algorithm\n",
    "\n",
    "#### Goal:\n",
    "Build a simple spam classifier based on NB that can detect and filter out unwanted email messages.\n",
    "\n",
    "#### Description:\n",
    "1. Collect a dataset containing labels (spam/non-spam) and the content of email messages, e.g., [Enron-Spam](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html) or [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) or [E-mail Spam](https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv) or similar.\n",
    "2. Prepare the data by tokenizing words and removing unnecessary punctuation marks.\n",
    "3. Implement NB that can classify messages as spam or non-spam based on the words present.\n",
    "4. Split the data into a training set and a test set (e.g., 70% for training, 30% for testing).\n",
    "5. Train the NB classifier on the training data.\n",
    "6. Test the classifier on the test data and evaluate its effectiveness using metrics: [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall), [f1-score](https://en.wikipedia.org/wiki/F-score), and [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
    "7. Analyze the results and present conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a933bb-c803-4e52-a562-e2a47944fb5f",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "### Classifier Based on MNB with N-grams\n",
    "\n",
    "#### Goal:\n",
    "Build a spam classifier using n-grams in combination with MNB to improve the effectiveness of email message classification.\n",
    "\n",
    "#### Description:\n",
    "1. Collect a dataset containing labels (spam/non-spam) and the content of email messages, e.g., [Enron-Spam](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html) or [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) or [E-mail Spam](https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv) or similar.\n",
    "2. Prepare the data by creating n-grams from the email content, i.e., unigrams, bigrams, trigrams.\n",
    "3. Implement MNB that can classify messages as spam or non-spam using n-grams as features.\n",
    "4. Split the data into a training set and a test set (e.g., 70% for training, 30% for testing).\n",
    "5. Train the MNB classifier on the training data using n-grams as features.\n",
    "6. Test the classifier on the test data and evaluate its effectiveness using metrics: [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall), [f1-score](https://en.wikipedia.org/wiki/F-score), and [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
    "7. Analyze the results and compare them with the results of the word-based classifier.\n",
    "8. Present conclusions regarding the effectiveness of the n-gram-based classifier and the impact of different types of n-grams on classification effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f57f4d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df83c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c340263",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5124b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.spam_prob = 0\n",
    "        self.spam_count = 0\n",
    "        self.ham_count = 0\n",
    "        self.spam_occurences = Counter()\n",
    "        self.ham_occurences = Counter()\n",
    "        self.vocabulary = set()\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        self.spam_prob = sum(1 for label in labels if label == 1) / len(labels)\n",
    "        for i in tqdm(range(len(data)), desc=\"Training\", total=len(labels)):\n",
    "            tokens = self._tokenize(data[i])\n",
    "            for token in set(tokens):\n",
    "                if labels[i]:\n",
    "                    self.spam_occurences[token] += 1\n",
    "                else:\n",
    "                    self.ham_occurences[token] += 1\n",
    "                self.vocabulary.add(token)\n",
    "            if labels[i]:\n",
    "                self.spam_count += 1\n",
    "            else:\n",
    "                self.ham_count += 1\n",
    "\n",
    "    def predict(self, data):\n",
    "        results = []\n",
    "        for document in tqdm(data, desc=\"Predicting\", total=len(data)):\n",
    "            tokens = self._tokenize(document)\n",
    "            results.append(self._decide_if_spam(tokens))\n",
    "        return results\n",
    "\n",
    "    def _tokenize(self, document):\n",
    "        tokens = word_tokenize(document)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        tokens = [token.translate(table) for token in tokens]\n",
    "        tokens = [token for token in tokens if len(token) >= 2]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _decide_if_spam(self, document):\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        spam_value = np.log(self.spam_prob)\n",
    "        ham_value = np.log(1 - self.spam_prob)\n",
    "\n",
    "        for token, count in Counter(document).items():\n",
    "            token_spam_prob = (1 + self.spam_occurences.get(token, 0)) / (vocab_size + self.spam_count)\n",
    "            token_ham_prob = (1 + self.ham_occurences.get(token, 0)) / (vocab_size + self.ham_count)\n",
    "\n",
    "            spam_value += np.log(token_spam_prob) * count\n",
    "            ham_value += np.log(token_ham_prob) * count\n",
    "        return spam_value >= ham_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1000e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNBClassifier:\n",
    "    def __init__(self, ngram_size=1):\n",
    "        self.ngram_size = ngram_size\n",
    "        self.spam_prob = 0\n",
    "        self.spam_tokens_count = 0\n",
    "        self.ham_tokens_count = 0\n",
    "        self.spam_occurences = Counter()\n",
    "        self.ham_occurences = Counter()\n",
    "        self.vocabulary = set()\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        self.spam_prob = sum(1 for label in labels if label == 1) / len(labels)\n",
    "        for i in tqdm(range(len(data)), desc=\"Training\", total=len(labels)):\n",
    "            tokens = self._tokenize(data[i])\n",
    "            for token in tokens:\n",
    "                if labels[i]:\n",
    "                    self.spam_occurences[token] += 1\n",
    "                    self.spam_tokens_count += 1\n",
    "                else:\n",
    "                    self.ham_occurences[token] += 1\n",
    "                    self.ham_tokens_count += 1\n",
    "                self.vocabulary.add(token)\n",
    "\n",
    "    def predict(self, data):\n",
    "        results = []\n",
    "        for document in tqdm(data, desc=\"Predicting\", total=len(data)):\n",
    "            tokens = self._tokenize(document)\n",
    "            results.append(self._decide_if_spam(tokens))\n",
    "        return results\n",
    "\n",
    "    def _tokenize(self, document):\n",
    "        tokens = word_tokenize(document)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        tokens = [token.translate(table) for token in tokens]\n",
    "        tokens = [token for token in tokens if len(token) >= 2]\n",
    "\n",
    "        for i in range(2, self.ngram_size + 1):\n",
    "            n_grams = list(ngrams(tokens, i))\n",
    "            tokens.extend([\" \".join(gram) for gram in n_grams])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _decide_if_spam(self, document):\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        spam_value = np.log(self.spam_prob)\n",
    "        ham_value = np.log(1 - self.spam_prob)\n",
    "\n",
    "        for token, count in Counter(document).items():\n",
    "            token_spam_prob = (1 + self.spam_occurences.get(token, 0)) / (vocab_size + self.spam_tokens_count)\n",
    "            token_ham_prob = (1 + self.ham_occurences.get(token, 0)) / (vocab_size + self.ham_tokens_count)\n",
    "\n",
    "            spam_value += np.log(token_spam_prob) * count\n",
    "            ham_value += np.log(token_ham_prob) * count\n",
    "        return spam_value >= ham_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5adba8",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e698ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(labels, predictions):\n",
    "    f1_score, precision, recall, accuracy = 0, 0, 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]:\n",
    "            if predictions[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predictions[i]:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    accuracy = (tp + tn) / len(labels)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return f1_score, accuracy, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f66f7e",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2639831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "spam_path = \"datasets/enron1/spam\"\n",
    "ham_path = \"datasets/enron1/ham\"\n",
    "\n",
    "for file in os.listdir(spam_path):\n",
    "    labels.append(1)\n",
    "    with open(os.path.join(spam_path, file), \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        data.append(file.read())\n",
    "for file in os.listdir(ham_path):\n",
    "    labels.append(0)\n",
    "    with open(os.path.join(ham_path, file), \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        data.append(file.read())\n",
    "\n",
    "train_indices, test_indices = train_test_split(range(len(data)), test_size=0.3, random_state=42)\n",
    "\n",
    "train_data = [data[i] for i in train_indices]\n",
    "train_labels = [labels[i] for i in train_indices]\n",
    "test_data = [data[i] for i in test_indices]\n",
    "test_labels = [labels[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ede3f",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93447c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3620/3620 [00:05<00:00, 643.64it/s]\n",
      "Training: 100%|██████████| 3620/3620 [00:05<00:00, 608.09it/s]\n",
      "Training: 100%|██████████| 3620/3620 [00:06<00:00, 558.69it/s]\n",
      "Training: 100%|██████████| 3620/3620 [00:08<00:00, 406.77it/s]\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "mnb = MNBClassifier()\n",
    "mnb2 = MNBClassifier(ngram_size=2)\n",
    "mnb3 = MNBClassifier(ngram_size=3)\n",
    "\n",
    "nb.train(train_data, train_labels)\n",
    "mnb.train(train_data, train_labels)\n",
    "mnb2.train(train_data, train_labels)\n",
    "mnb3.train(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b85541",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4a8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1552/1552 [00:04<00:00, 358.19it/s]\n",
      "Predicting: 100%|██████████| 1552/1552 [00:04<00:00, 370.57it/s]\n",
      "Predicting: 100%|██████████| 1552/1552 [00:04<00:00, 334.73it/s]\n",
      "Predicting: 100%|██████████| 1552/1552 [00:08<00:00, 181.51it/s]\n"
     ]
    }
   ],
   "source": [
    "nb_predictions = nb.predict(test_data)\n",
    "mnb_predictions = [mnb.predict(test_data), mnb2.predict(test_data), mnb3.predict(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f880c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier performance on test data:\n",
      "\tf1_score: 0.698, accuracy: 0.860\n",
      "\trecall: 0.536, precision: 1.000\n",
      "\n",
      "MNB Classifier performance on test data:\n",
      "\tn = 1\n",
      "\t\tf1_score: 0.965, accuracy: 0.979\n",
      "\t\trecall: 0.966, precision: 0.964\n",
      "\tn = 2\n",
      "\t\tf1_score: 0.975, accuracy: 0.985\n",
      "\t\trecall: 0.987, precision: 0.963\n",
      "\tn = 3\n",
      "\t\tf1_score: 0.945, accuracy: 0.965\n",
      "\t\trecall: 0.989, precision: 0.904\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Classifier performance on test data:\")\n",
    "f1_score, accuracy, recall, precision = evaluate_performance(test_labels, nb_predictions)\n",
    "print(f\"\\tf1_score: {f1_score:.3f}, accuracy: {accuracy:.3f}\")\n",
    "print(f\"\\trecall: {recall:.3f}, precision: {precision:.3f}\")\n",
    "    \n",
    "print(\"\\nMNB Classifier performance on test data:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\tn = {i+1}\")\n",
    "    f1_score, accuracy, recall, precision = evaluate_performance(test_labels, mnb_predictions[i])\n",
    "    print(f\"\\t\\tf1_score: {f1_score:.3f}, accuracy: {accuracy:.3f}\")\n",
    "    print(f\"\\t\\trecall: {recall:.3f}, precision: {precision:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
